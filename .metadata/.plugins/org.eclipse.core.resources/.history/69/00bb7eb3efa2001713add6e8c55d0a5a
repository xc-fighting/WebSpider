package crawlerControl;

import edu.uci.ics.crawler4j.crawler.CrawlConfig;
import edu.uci.ics.crawler4j.crawler.CrawlController;
import edu.uci.ics.crawler4j.fetcher.PageFetcher;
import edu.uci.ics.crawler4j.robotstxt.RobotstxtConfig;
import edu.uci.ics.crawler4j.robotstxt.RobotstxtServer;

public class Controller {

	public static void main(String[] args){
		String crawlStorageFolder="/data/crawl";
		int numberOfCrawler=7;
		CrawlConfig config=new CrawlConfig();
		config.setCrawlStorageFolder(crawlStorageFolder);
		
		PageFetcher pageFetcher=new PageFetcher(config);
		RobotstxtConfig robottxtConfig=new RobotstxtConfig();
		RobotstxtServer robotstxtServer=new RobotstxtServer(robottxtConfig,pageFetcher);
		try {
			CrawlController controller=new CrawlController(config,pageFetcher,robotstxtServer);
			controller.addSeed("http://www.viterbit.usc.edu/");
			controller.start(Controller.class, numberOfCrawler);
		} catch (Exception e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		
	}
}
